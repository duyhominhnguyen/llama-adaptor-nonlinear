# ğŸ¦™ LLAMA-Adaptor: Linear and Non-Linear Prompt-Tuning Approaches

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Paper](https://img.shields.io/badge/arXiv-Paper-red)](https://arxiv.org/abs/2502.03029)
[![Presentation](https://img.shields.io/badge/YouTube-Presentation-red)](https://www.youtube.com/watch?v=ZWuAbjE0cCU)
[![Code](https://img.shields.io/badge/Code-PyTorch-green)](#)

> Official repository for our paper: **"On Zero-Initialized Attention: Optimal Prompt and Gating Factor Estimation", ICML 2025**

---

## ğŸ“ Introduction

The LLaMA-Adapter has recently emerged as an efficient fine-tuning technique for LLaMA models, leveraging zero-initialized attention to stabilize training and enhance performance. However, despite its empirical success, **the theoretical foundations of zero-initialized attention remain largely unexplored**. 

In this paper, we **(i) provide a rigorous theoretical analysis, establishing a connection between zero-initialized attention and mixture-of-expert models**. We **(ii) prove that both linear and non-linear prompts**, along with gating functions, can be optimally estimated, with non-linear prompts offering greater flexibility for future applications. Empirically, we validate our findings on the open LLM benchmarks, demonstrating that non-linear prompts outperform linear ones. Notably, even with limited training data, both prompt types consistently surpass vanilla attention, highlighting the robustness and adaptability of zero-initialized attention.

<p align="center">
<img src="https://github.com/duyhominhnguyen/llama-adaptor-nonlinear/blob/main/figures/fig_1_overview.png" alt="Alt text" width="400"/>
</p>

<img src="figures/presentation.gif" alt="your_alternative_text" width="your_width" height="your_height" loop=infinite>

---

## ğŸ” Key Features

- âœ… Parameter-efficient fine-tuning with linear/non-linear prompts combined with zero-initialized attention
- âš¡ Fast convergence on Alpaca dataset
- ğŸ§© Modular integration into existing LLAMA architectures
- ğŸ“‰ Outperform the original prefix-tuning method and non-linear prompt further enhance the performance

---

## ğŸ“Š Main Experiments

### ğŸ§ª Evaluation Benchmarks

We evaluate our models the following benchmarks: ARC, HellaSwag, MMLU, TruthfulQA, which includes four diverse tasks to assess the generative, reasoning, and factual capabilities of LLMs. We reproduce all the results of the baselines in our paper.

- **ARC (AI2 Reasoning Challenge)** â€“ Easy (`ARC-eas`) and Challenge (`ARC-cha`) subsets [(Clark et al., 2018)]  
  > Evaluates commonsense and scientific reasoning.
  
- **HellaSwag** [(Zellers et al., 2019)]  
  > Tests the model's ability to complete sentences using commonsense inference.
  
- **MMLU (Massive Multitask Language Understanding)** [(Hendrycks et al., 2020)]  
  > Measures knowledge across 57 academic subjects.

- **TruthfulQA** [(Lin et al., 2021)]  
  > Measures whether model outputs are truthful rather than just plausible.

**Evaluation Settings**:
- **Zero-shot** evaluation: ARC, MMLU, and TruthfulQA  
- **10-shot** evaluation: HellaSwag  
  > (n-shot means using n instruction-following examples in the prompt.)

---

### ğŸ—ï¸ Architectures & Training Setup

We conduct experiments on two LLaMA model sizes:

| Model       | Transformer Layers | Prompt Length (L) | Prompt Layers (K) |
|-------------|--------------------|--------------------|--------------------|
| LLaMA-7B    | 32                 | 10                 | Last 30 layers     |
| LLaMA-13B   | 40                 | 10                 | Last 38 layers     |

**Training Configuration**:
- GPUs: 4 Ã— A100
- Epochs: 5 (including 2 warm-up epochs)
- Total Batch Size: 64
- Learning Rate: 0.009
- Weight Decay: 0.02
- Type of Non-Linear MLP-2 layers (--typ_act): hypernetwork
- Type of Activation Function in Non-Linear MLP-2 layers (--hid_acti_func): relu
- Hidden Dim of Non-Linear MLP-2 layers (--hidden_dim): 64
- Dataset: [Alpaca (Taori et al., 2023)](https://github.com/tatsu-lab/stanford_alpaca)

**Optimizer**: AdamW  
**Fine-tuning Method**: Zero-Initialized Attention with Linear/Non-Linear Prompts

---

### ğŸ“¦ Model Checkpoints

| Model                                  | Description                                | Download Link |
|----------------------------------------|--------------------------------------------|---------------|
| `7B-non-linear-prompt`                            | LLaMA-7B, Zero-Initialized Attention with non-linear prompt                   | [Link](https://drive.google.com/drive/folders/1pHYuPxskfaBEv9qy7Wl-8XL6dG-xY2vv?usp=sharing)     |
| `7B-linear-prompt`                           | LLaMA-7B, Zero-Initialized Attention with linear prompt                   | [Link](https://drive.google.com/drive/folders/1pHYuPxskfaBEv9qy7Wl-8XL6dG-xY2vv?usp=sharing)     |
| `7B-random-init-prompt`                         | LLaMA-7B, Prefix-Tuning with Conventional Attention                   | [Link](https://drive.google.com/drive/folders/1pHYuPxskfaBEv9qy7Wl-8XL6dG-xY2vv?usp=sharing)     |
| `13B-non-linear-prompt`                            | LLaMA-13B, Zero-Initialized Attention with non-linear prompt                   | [Link](https://drive.google.com/drive/folders/1pHYuPxskfaBEv9qy7Wl-8XL6dG-xY2vv?usp=sharing)     |
| `13B-linear-prompt`                           | LLaMA-13B, Zero-Initialized Attention with linear prompt                   | [Link](https://drive.google.com/drive/folders/1pHYuPxskfaBEv9qy7Wl-8XL6dG-xY2vv?usp=sharing)     |
| `13B-random-init-prompt`                         | LLaMA-13B, Prefix-Tuning with Conventional Attention                   | [Link](https://drive.google.com/drive/folders/1pHYuPxskfaBEv9qy7Wl-8XL6dG-xY2vv?usp=sharing)     |

---

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/duyhominhnguyen/llama-adaptor-nonlinear.git
cd llama-adaptor-nonlinear
```

### Repo Structures
```
llama-adaptor/
â”œâ”€â”€ figures                                     # Figures in the paper
â”œâ”€â”€ README.md                                   # Project overview and usage
â”œâ”€â”€ LICENSE                                     # MIT license
â”œâ”€â”€ requirements.txt                            # Required Python packages
â”œâ”€â”€ setup.py                                    # Optional: Install as a package
â”œâ”€â”€ setup.cfg
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ generate.sh
â”œâ”€â”€ example.py                                  # Code for running evaluation on ARC, MMLU, and TruthfulQA
â”œâ”€â”€ hellaswag_check.py                          
â”œâ”€â”€ hellaswag_check.sh                          # Script for running evaluation on HellaSwag
â”œâ”€â”€ alpaca_data.json                            # Dataset for fine-tuning the LLaMA-Adapter
â”œâ”€â”€ alpaca_finetuning_v1/                       # Contain code and script for fine-tuning LLaMA-Adapter
â”‚   â”œâ”€â”€ llama/
|   |   â”œâ”€â”€ generation.py
|   |   â”œâ”€â”€ model.py
|   |   â””â”€â”€ tokenizer.py
|   â”œâ”€â”€ util/
|   â”œâ”€â”€ LLaMA-7B/                               # These files are not included, see the instructions below
|   |   â”œâ”€â”€ checklist.chk
|   |   â”œâ”€â”€ consolidated.00.pth
|   |   â”œâ”€â”€ params.json
|   |   â””â”€â”€ tokenizer.model
|   â”œâ”€â”€ engine_finetuning.py
|   â”œâ”€â”€ extract_adapter_from_checkpoint.py      # Extract adapter weight
|   â”œâ”€â”€ finetune_llama.sh                       # Script finetune
|   â”œâ”€â”€ finetuning.py
â”‚   â””â”€â”€ models_llama_adapter.py
â”œâ”€â”€ llama/                                      # Example shell scripts for training/eval
|   â”œâ”€â”€ generation.py
|   â”œâ”€â”€ model.py
|   â””â”€â”€ tokenizer.py
â”œâ”€â”€ lm-evaluation-harness/                      # Folder Library for evaluating HellaSwag benchmark
â”œâ”€â”€ utils/                                      # Helper functions: quantization.py
â”‚   â”œâ”€â”€ quantization.py 
â””â”€â”€ docs/                                       # (Optional) Additional documentation
    â””â”€â”€ architecture.png
```

### 2. Set Up

- Create the Python environment by following these steps:

```bash
conda create -n llama_adaptor -y python=3.9
conda activate llama_adaptor

# install dependency and llama-adapter
pip install -r requirements.txt
pip install -e .

# install lm-evaluation-harness library for HellaSwag evaluation
cd lm-evaluation-harness/
pip install -e .
```

- Download the LLaMA-7B model weights from this [link](https://huggingface.co/nyanko7/LLaMA-7B/tree/main), then put the files into the `alpaca_finetuning_v1/LLaMA-7B` folder.

### 3. Training

To fine-tune LLaMA using linear or non-linear prompts:

```bash
# Linear prompt tuning on LLaMA
cd alpaca_finetuning_v1/
bash finetune_llama_linear.sh
# or
typ_act=identity
typ_gate=random
hid_acti_func=none
hidden_dim=0

torchrun --nnodes=1 --nproc_per_node=4 --master_port=25012 finetuning.py \
    --model Llama7B_adapter \
    --llama_model_path ../LLaMA-${name} \
    --data_path ../alpaca_data.json \
    --adapter_layer ${adapter_layer} \
    --adapter_len 10 \
    --max_seq_len 512 \
    --batch_size ${batch_size} \
    --epochs ${epoch} \
    --warmup_epochs 2 \
    --blr 9e-3 \
    --weight_decay 0.02 \
    --typ_act ${typ_act} \
    --typ_gate ${typ_gate} \
    --hid_acti_func ${hid_acti_func}\
    --hidden_dim ${hidden_dim}\
    --output_dir ./checkpoint_adapter_layer${adapter_layer}_${typ_act}${hidden_dim}${hid_acti_func}_random_init${random_init}_batchsize${batch_size}_epoch${epoch}_${name}_test/

# Non-linear prompt tuning on LLaMA
cd alpaca_finetuning_v1/
bash finetune_llama_non_linear.sh
# or
typ_act=hypermodel
typ_gate=random
hid_acti_func=relu
hidden_dim=64

torchrun --nnodes=1 --nproc_per_node=4 --master_port=25012 finetuning.py \
    --model Llama7B_adapter \
    --llama_model_path ../LLaMA-${name} \
    --data_path ../alpaca_data.json \
    --adapter_layer ${adapter_layer} \
    --adapter_len 10 \
    --max_seq_len 512 \
    --batch_size ${batch_size} \
    --epochs ${epoch} \
    --warmup_epochs 2 \
    --blr 9e-3 \
    --weight_decay 0.02 \
    --typ_act ${typ_act} \
    --typ_gate ${typ_gate} \
    --hid_acti_func ${hid_acti_func}\
    --hidden_dim ${hidden_dim}\
    --output_dir ./checkpoint_adapter_layer${adapter_layer}_${typ_act}${hidden_dim}${hid_acti_func}_random_init${random_init}_batchsize${batch_size}_epoch${epoch}_${name}_test/
```

After training, the loss curves for the three settings â€” **non-linear prompts**, **linear prompts**, and **random-initialized prompts** â€” have the following shapes:

<p align="center">
<img src="https://github.com/duyhominhnguyen/llama-adaptor-nonlinear/blob/main/figures/loss_curve.png" alt="Alt text" width="400"/>
</p>

To plot the loss curve, use the log file located in the checkpoint folder in combination with the `draw_curve.py` script found in the `./utils` directory.

### 4. Extracting Adapter Weight before Inference

After finishing training on Alpaca dataset, we first extract weight of adapter, then feed into inference script.

```bash
cd alpaca_finetuning_v1/
bash extract_adapter.sh
# or
python extract_adapter_from_checkpoint.py \
     --folder ./checkpoint_adapter_layer30_hypermodel64relu_random_initFalse_batchsize16_epoch5_7B_test \
```

### 5. Inference

You can download adapter weights of non-linear prompt setting in [weights] and put into the "example_weight" folder to run the inference scripts.

```bash
# Zero-shot evaluation on ARC
torchrun --nnodes=1 --nproc_per_node=1 --master_port=25010 example.py \
      --typeQues arc \
      --typ_act hypermodel \
      --hid_acti_func relu \
      --batch_size_per_iter 32 \
      --random_init False \
      --max_seq_len 2048 \
      --ckpt_dir ./alpaca_finetuning_v1/LLaMA-7B\
      --tokenizer_path ./alpaca_finetuning_v1/LLaMA-7B/tokenizer.model \
      --adapter_path ./example_weight/non_linear_prompt_7B.pth

# 10-shot evaluation on HellaSwag
bash hellaswag_check.sh
# or
python hellaswag_check.py \
      --ckpt_dir ./alpaca_finetuning_v1/LLaMA-7B \
      --adapter_path ./example_weight/non_linear_prompt_7B.pth \
      --typ_act hypermodel \
      --hid_acti_func relu \
      --random_init False
```

To test with linear models, set the arguments `--typ_act` to `identity` and `--hid_acti_func` to `none`. If you encounter out-of-memory errors, try reducing `--batch_size_per_iter` and `--max_seq_len`.

## ğŸ“š Citation

If you use this codebase or find our work helpful, please consider citing:

```bibtex
@article{diep2025zero,
  title={On Zero-Initialized Attention: Optimal Prompt and Gating Factor Estimation},
  author={Diep, Nghiem T and Nguyen, Huy and Nguyen, Chau and Le, Minh and Nguyen, Duy MH and Sonntag, Daniel and Niepert, Mathias and Ho, Nhat},
  journal={International Conference on Machine Learning (ICML)},
  year={2025}
}



